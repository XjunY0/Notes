## 优化器的用法

优化器 (optimizer) 是编译 Keras 模型的所需的两个参数之一：

```
from keras import optimizers

model = Sequential()
model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
model.add(Activation('softmax'))

sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)
```

你可以先实例化一个优化器对象，然后将它传入 `model.compile()`，像上述示例中一样， 或者你可以通过名称来调用优化器。在后一种情况下，将使用优化器的默认参数。

```
# 传入优化器名称: 默认参数将被采用
model.compile(loss='mean_squared_error', optimizer='sgd')
```

----------

## Keras 优化器的公共参数

参数 `clipnorm` 和 `clipvalue` 能在所有的优化器中使用，用于控制梯度裁剪（Gradient Clipping）：

```
from keras import optimizers

# 所有参数梯度将被裁剪，让其 l2 范数最大为 1：g * 1 / max(1, l2_norm)
sgd = optimizers.SGD(lr=0.01, clipnorm=1.)
```

```
from keras import optimizers

# 所有参数 d 梯度将被裁剪到数值范围内：
# 最大值 0.5
# 最小值 -0.5
sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)
```

----------

[[source]](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L164)

### SGD

```
keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)
```

随机梯度下降优化器。

包含扩展功能的支持：

-   动量（momentum）优化,
-   学习率衰减（每次参数更新后）
-   Nestrov 动量 (NAG) 优化

**参数**

-   **learning_rate**: float >= 0. 学习率。
-   **momentum**: float >= 0. 参数，用于加速 SGD 在相关方向上前进，并抑制震荡。
-   **nesterov**: boolean. 是否使用 Nesterov 动量。

----------

[[source]](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L229)

### RMSprop

```
keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)
```

RMSProp 优化器。

建议使用优化器的默认参数 （除了学习率，它可以被自由调节）

这个优化器通常是训练循环神经网络 RNN 的不错选择。

**参数**

-   **learning_rate**: float >= 0. 学习率。
-   **rho**: float >= 0. RMSProp 梯度平方的移动均值的衰减率。

**参考文献**

-   [rmsprop: Divide the gradient by a running average of its recent magnitude](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)

----------

[[source]](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L303)

### Adagrad

```
keras.optimizers.Adagrad(learning_rate=0.01)
```

Adagrad 优化器。

Adagrad 是一种具有特定参数学习率的优化器，它根据参数在训练期间的更新频率进行自适应调整。参数接收的更新越多，更新越小。

建议使用优化器的默认参数。

**参数**

-   **learning_rate**: float >= 0. 学习率。

**参考文献**

-   [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)

----------

[[source]](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L376)

### Adadelta

```
keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95)
```

Adadelta 优化器。

Adadelta 是 Adagrad 的一个具有更强鲁棒性的的扩展版本，它不是累积所有过去的梯度，而是根据渐变更新的移动窗口调整学习速率。 这样，即使进行了许多更新，Adadelta 仍在继续学习。 与 Adagrad 相比，在 Adadelta 的原始版本中，您无需设置初始学习率。 在此版本中，与大多数其他 Keras 优化器一样，可以设置初始学习速率和衰减因子。

建议使用优化器的默认参数。

**参数**

-   **learning_rate**: float >= 0. 初始学习率，默认为 1。建议保留默认值。
-   **rho**: float >= 0. Adadelta 梯度平方移动均值的衰减率。

**参考文献**

-   [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)

----------

[[source]](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L467)

### Adam

```
keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
```

Adam 优化器。

默认参数遵循原论文中提供的值。

**参数**

-   **learning_rate**: float >= 0. 学习率。
-   **beta_1**: float, 0 < beta < 1. 通常接近于 1。
-   **beta_2**: float, 0 < beta < 1. 通常接近于 1。
-   **amsgrad**: boolean. 是否应用此算法的 AMSGrad 变种，来自论文 "On the Convergence of Adam and Beyond"。

**参考文献**

-   [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)
-   [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)

----------

[[source]](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L567)

### Adamax

```
keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999)
```

Adamax 优化器，来自 Adam 论文的第七小节.

它是Adam算法基于无穷范数（infinity norm）的变种。 默认参数遵循论文中提供的值。

**参数**

-   **learning_rate**: float >= 0. 学习率。
-   **beta_1**: floats, 0 < beta < 1. 通常接近于 1。
-   **beta_2**: floats, 0 < beta < 1. 通常接近于 1。

**参考文献**

-   [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)

----------

[[source]](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L645)

### Nadam

```
keras.optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)
```

Nesterov 版本 Adam 优化器。

正像 Adam 本质上是 RMSProp 与动量 momentum 的结合， Nadam 是采用 Nesterov momentum 版本的 Adam 优化器。

默认参数遵循论文中提供的值。 建议使用优化器的默认参数。

**参数**

-   **learning_rate**: float >= 0. 学习率。
-   **beta_1**: floats, 0 < beta < 1. 通常接近于 1。
-   **beta_2**: floats, 0 < beta < 1. 通常接近于 1。

**参考文献**

-   [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)
-   [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)

选择优化器时，要考虑问题的类型、数据集的特点、模型架构以及训练速度和稳定性等因素。以下是 Keras 中几种常用优化器及其适用场景的详细解释：

### 1. SGD (Stochastic Gradient Descent)

python

复制代码

`keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)`

-   **适用场景**：经典的梯度下降优化器，适用于简单或线性问题，以及不需要复杂优化策略的任务。
-   **特点**：
    -   **学习率 (learning_rate)**：控制步长。
    -   **动量 (momentum)**：加速收敛并减少震荡。
    -   **Nesterov 动量 (nesterov)**：在传统动量的基础上进一步提高优化速度和稳定性。
-   **推荐使用**：当数据量较大且需要稳定收敛时，尤其是计算资源有限的情况下。

### 2. RMSprop

python

复制代码

`keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)`

-   **适用场景**：推荐用于循环神经网络 (RNN) 和需要自适应学习率的任务。
-   **特点**：
    -   **Rho**：RMSProp 梯度平方的移动均值的衰减率。
    -   自动调整每个参数的学习率，适应不同特征的稀疏性和密集性。
-   **推荐使用**：当训练 RNN 或处理非平稳目标时，可以使用 RMSprop 获得更好的性能。

### 3. Adagrad

python

复制代码

`keras.optimizers.Adagrad(learning_rate=0.01)`

-   **适用场景**：适用于稀疏数据和需要自适应学习率的任务。
-   **特点**：
    -   根据参数更新的频率调整学习率，更新频率高的参数学习率会减少。
    -   不适合长时间训练，因为学习率会不断降低。
-   **推荐使用**：当处理稀疏数据或需要对每个参数进行自适应学习时，Adagrad 是一个不错的选择。

### 4. Adadelta

python

复制代码

`keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95)`

-   **适用场景**：Adagrad 的改进版本，适用于大规模数据和需要长期训练的任务。
-   **特点**：
    -   使用一个移动窗口调整学习率，保持长期学习能力。
    -   不需要设置初始学习率，默认学习率通常表现良好。
-   **推荐使用**：当处理长时间训练任务或需要自适应学习率时，Adadelta 是一个可靠的选择。

### 5. Adam

python

复制代码

`keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)`

-   **适用场景**：广泛使用的优化器，适用于大多数深度学习任务。
-   **特点**：
    -   结合了动量 (momentum) 和 RMSprop 的优点。
    -   使用两个超参数 beta_1 和 beta_2 控制一阶和二阶矩估计。
    -   可以选择 AMSGrad 变种，提供更好的收敛性。
-   **推荐使用**：当不确定使用哪个优化器时，Adam 通常是一个安全且有效的选择。

### 6. Adamax

python

复制代码

`keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999)`

-   **适用场景**：Adam 的变种，适用于大规模数据和模型。
-   **特点**：
    -   基于无穷范数的 Adam 变种。
    -   对于大型数据集或大模型更为稳定。
-   **推荐使用**：当训练大型模型时，可以考虑使用 Adamax 以获得更好的数值稳定性。

### 7. Nadam

python

复制代码

`keras.optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)`

-   **适用场景**：Adam 的变种，结合了 Nesterov 动量，适用于需要更快收敛的任务。
-   **特点**：
    -   结合了 Nesterov 动量和 Adam 的优点。
    -   提供更快的收敛速度和更好的性能。
-   **推荐使用**：当需要快速收敛并希望结合 Nesterov 动量的优点时，可以选择 Nadam。

### 总结

-   **简单任务**：使用 `SGD`，可以通过增加 `momentum` 和 `nesterov` 提高性能。
-   **稀疏数据**：使用 `Adagrad` 或 `RMSprop`。
-   **需要长期训练**：使用 `Adadelta`。
-   **通用优化**：使用 `Adam` 或其变种 `Adamax` 和 `Nadam`。
-   **循环神经网络**：使用 `RMSprop` 或 `Adam`。

选择优化器时，可以先尝试 Adam，因为它在大多数情况下表现良好。如果 Adam 的效果不理想，可以根据任务特点选择其他优化器进行调优。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTczOTY3NjQyMV19
-->