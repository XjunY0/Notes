## 激活函数的用法

激活函数可以通过设置单独的 `Activation` 层实现，也可以在构造层对象时通过传递 `activation` 参数实现：

```py
from keras.layers import Activation, Dense

model.add(Dense(64))
model.add(Activation('tanh'))
```

等价于：

```py
model.add(Dense(64, activation='tanh'))
```

你也可以通过传递一个逐元素运算的 Theano/TensorFlow/CNTK 函数来作为激活函数：

```py
from keras import backend as K

model.add(Dense(64, activation=K.tanh))
model.add(Activation(K.tanh))
```

## 预定义激活函数

### elu

```py
keras.activations.elu(x, alpha=1.0)
```

指数线性单元。

**参数**

-   **x**：输入张量。
-   **alpha**：一个标量，表示负数部分的斜率。

**返回**

线性指数激活：如果 `x > 0`，返回值为 `x`；如果 `x < 0` 返回值为 `alpha * (exp(x)-1)`

**参考文献**

-   [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289)

----------

### softmax

```
keras.activations.softmax(x, axis=-1)
```

Softmax 激活函数。

**参数**

-   **x**：输入张量。
-   **axis**：整数，代表 softmax 所作用的维度。

**返回**

softmax 变换后的张量。

**异常**

-   **ValueError**：如果 `dim(x) == 1`。

----------

### selu

```
keras.activations.selu(x)
```

可伸缩的指数线性单元（SELU）。

SELU 等同于：`scale * elu(x, alpha)`，其中 alpha 和 scale 是预定义的常量。只要正确初始化权重（参见 `lecun_normal` 初始化方法）并且输入的数量「足够大」（参见参考文献获得更多信息），选择合适的 alpha 和 scale 的值，就可以在两个连续层之间保留输入的均值和方差。

**参数**

-   **x**: 一个用来用于计算激活函数的张量或变量。

**返回**

可伸缩的指数线性激活：`scale * elu(x, alpha)`。

**注意**

-   与「lecun_normal」初始化方法一起使用。
-   与 dropout 的变种「AlphaDropout」一起使用。

**参考文献**

-   [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)

----------

### softplus

```
keras.activations.softplus(x)
```

Softplus 激活函数。

**参数**

-   **x**: 输入张量。

**返回**

Softplus 激活：`log(exp(x) + 1)`。

----------

### softsign

```
keras.activations.softsign(x)
```

Softsign 激活函数。

**参数**

-   **x**: 输入张量。

**返回**

Softsign 激活：`x / (abs(x) + 1)`。

----------

### relu

```
keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)
```

整流线性单元。

使用默认值时，它返回逐元素的 `max(x, 0)`。

否则，它遵循：

-   如果 `x >= max_value`：`f(x) = max_value`，
-   如果 `threshold <= x < max_value`：`f(x) = x`，
-   否则：`f(x) = alpha * (x - threshold)`。

**参数**

-   **x**: 输入张量。
-   **alpha**：负数部分的斜率。默认为 0。
-   **max_value**：输出的最大值。
-   **threshold**: 浮点数。Thresholded activation 的阈值值。

**返回**

一个张量。

----------

### tanh

```
keras.activations.tanh(x)
```

双曲正切激活函数。

**参数**

-   **x**: 输入张量。

**返回**

双曲正切激活函数: `tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`

----------

### sigmoid

```
sigmoid(x)
```

Sigmoid 激活函数。

**参数**

-   **x**: 输入张量.

**返回**

Sigmoid激活函数: `1 / (1 + exp(-x))`.

----------

### hard_sigmoid

```
hard_sigmoid(x)
```

Hard sigmoid 激活函数。

计算速度比 sigmoid 激活函数更快。

**参数**

-   **x**: 输入张量。

**返回**

Hard sigmoid 激活函数：

-   如果 `x < -2.5`，返回 `0`。
-   如果 `x > 2.5`，返回 `1`。
-   如果 `-2.5 <= x <= 2.5`，返回 `0.2 * x + 0.5`。

----------

### exponential

```
keras.activations.exponential(x)
```

自然数指数激活函数。

----------

### linear

```
keras.activations.linear(x)
```

线性激活函数（即不做任何改变）

**参数**

-   **x**: 输入张量。

**返回**

输入张量，不变。

## 高级激活函数

对于 Theano/TensorFlow/CNTK 不能表达的复杂激活函数，如含有可学习参数的激活函数，可通过[高级激活函数](https://keras-zh.readthedocs.io/layers/advanced-activations/)实现，可以在 `keras.layers.advanced_activations` 模块中找到。 这些高级激活函数包括 `PReLU` 和 `LeakyReLU`。

选择激活函数是设计神经网络时的关键一步，不同的激活函数适用于不同类型的任务和网络层。下面是一些常见的激活函数及其选择依据：

### 1. ReLU (Rectified Linear Unit)


`keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)`

-   **适用场景**：深层神经网络，特别是卷积神经网络 (CNN) 和前馈神经网络 (FFNN)。
-   **特点**：
    -   快速收敛。
    -   计算简单：`max(x, 0)`。
    -   处理梯度消失问题。
-   **缺点**：可能会导致神经元死亡（输出始终为0）。
-   **变种**：Leaky ReLU、Parametric ReLU (PReLU)。

### 2. ELU (Exponential Linear Unit)



`keras.activations.elu(x, alpha=1.0)`

-   **适用场景**：深度网络，需要更强的负值能力。
-   **特点**：
    -   负值输出有助于均值接近零，使梯度更加健壮。
    -   更快的收敛速度。
-   **参考文献**：Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)。

### 3. SELU (Scaled Exponential Linear Unit)


`keras.activations.selu(x)`

-   **适用场景**：自正则化神经网络 (SNN)。
-   **特点**：
    -   自动标准化。
    -   需与 `lecun_normal` 初始化和 `AlphaDropout` 一起使用。
-   **参考文献**：Self-Normalizing Neural Networks。

### 4. Softmax


`keras.activations.softmax(x, axis=-1)`

-   **适用场景**：多类分类任务的输出层。
-   **特点**：
    -   转换为概率分布。
    -   确保输出和为1。
-   **使用**：多分类任务（分类数 > 2）。

### 5. Sigmoid

`keras.activations.sigmoid(x)`

-   **适用场景**：二分类任务的输出层。
-   **特点**：
    -   输出在0和1之间。
    -   有概率解释。
-   **缺点**：容易导致梯度消失。

### 6. Tanh


`keras.activations.tanh(x)`

-   **适用场景**：隐藏层的非线性激活函数。
-   **特点**：
    -   输出在-1和1之间。
    -   中心对称，零均值。
-   **缺点**：容易导致梯度消失。

### 7. Softplus


`keras.activations.softplus(x)`

-   **适用场景**：平滑的替代ReLU。
-   **特点**：
    -   平滑且可微。
    -   输出大于0。
-   **公式**：`log(exp(x) + 1)`。

### 8. Softsign


`keras.activations.softsign(x)`

-   **适用场景**：平滑替代tanh。
-   **特点**：
    -   输出在-1和1之间。
    -   更平滑的渐变。
-   **公式**：`x / (abs(x) + 1)`。

### 9. Hard Sigmoid

`keras.activations.hard_sigmoid(x)`

-   **适用场景**：需要计算更快的sigmoid替代。
-   **特点**：
    -   近似 sigmoid，计算更快。
    -   适用于移动设备和嵌入式系统。
-   **公式**：分段线性近似。

### 10. Exponential

`keras.activations.exponential(x)`

-   **适用场景**：需要快速增长的激活函数。
-   **特点**：
    -   对输入进行指数变换。
-   **公式**：`exp(x)`。

### 11. Linear





<!--stackedit_data:
eyJoaXN0b3J5IjpbMTEwMjQwODg4NiwtMTYwMTg2MDFdfQ==
-->