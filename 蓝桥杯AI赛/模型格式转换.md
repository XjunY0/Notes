## onnx

`onnx`（Open Neural Network Exchange）是一个开放格式，用于机器学习模型的跨框架互操作性。使用 `onnx`，你可以在不同的深度学习框架之间导出和导入模型。

下面是一些常见的 `onnx` 用法示例，包括如何安装 `onnx` 和相关库，如何导出模型到 `onnx` 格式，以及如何加载和使用 `onnx` 模型。

### 将Pytorch模型导出为 `onnx` 格式

假设我们使用 PyTorch 训练了一个简单的模型，现在我们将其导出为 `onnx` 格式。
~~~py
import torch
import torch.nn as nn
import torch.onnx

# 定义一个简单的模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 5)

    def forward(self, x):
        return self.fc(x)

# 初始化模型并生成一个输入张量
model = SimpleModel()
dummy_input = torch.randn(1, 10)

# 导出模型到 onnx 格式
torch.onnx.export(model, dummy_input, "simple_model.onnx", verbose=True)
~~~

~~~py
torch.onnx.export(model, args, f, export_params=True, verbose=False, training=torch.onnx.TrainingMode.EVAL, input_names=None, output_names=None, operator_export_type=None, opset_version=None, do_constant_folding=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False, onnx_shape_inference=False)
~~~

#### 参数说明

-   `model`：要导出的 PyTorch 模型。
-   `args`：模型的输入张量，或输入张量的元组。
-   `f`：要保存的文件名或文件对象。
-   `export_params`：是否导出模型参数。默认为 `True`。
-   `verbose`：是否打印导出过程的详细信息。默认为 `False`。
-   `training`：导出模型的模式，可以是 `TrainingMode.EVAL`（评估模式）或 `TrainingMode.TRAINING`（训练模式）。
-   `input_names`：输入节点的名称列表。
-   `output_names`：输出节点的名称列表。
-   `operator_export_type`：导出类型。
-   `opset_version`：导出的 ONNX opset 版本。
-   `do_constant_folding`：是否执行常量折叠优化。默认为 `True`。
-   `dynamic_axes`：动态轴的定义，用于支持可变长度输入。
-   `keep_initializers_as_inputs`：是否将初始化器保留为输入。默认为 `None`。
-   `custom_opsets`：自定义 opset 版本。
-   `enable_onnx_checker`：是否启用 ONNX 检查器。默认为 `True`。
-   `use_external_data_format`：是否使用外部数据格式。默认为 `False`。
-   `onnx_shape_inference`：是否在导出时执行 ONNX 形状推理。默认为 `False`。

### 加载和运行 `onnx` 模型

使用 `onnxruntime` 加载和运行 `onnx` 模型。
~~~py
import onnxruntime as ort
import numpy as np

# 加载 onnx 模型
ort_session = ort.InferenceSession("simple_model.onnx")

# 创建输入数据
input_data = np.random.randn(1, 10).astype(np.float32)

# 运行模型
outputs = ort_session.run(None, {"input.1": input_data})
print(outputs)
~~~

### 验证 `onnx` 模型

使用 `onnx` 库来验证模型的结构和正确性。
~~~py
import onnx

# 加载 onnx 模型
model = onnx.load("simple_model.onnx")

# 检查模型是否有效
onnx.checker.check_model(model)

# 打印模型的图形信息
print(onnx.helper.printable_graph(model.graph))
~~~

### 将其他框架的模型转换为 `onnx`

许多深度学习框架都支持将模型导出为 `onnx` 格式。例如，将 TensorFlow 模型导出为 `onnx`：
~~~py
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tf2onnx

# 定义一个简单的模型
model = Sequential([Dense(5, input_shape=(10,), activation='relu')])

# 将模型保存为 SavedModel 格式
model.save("simple_model")

# 将 SavedModel 转换为 onnx 格式
!python -m tf2onnx.convert --saved-model simple_model --output simple_model.onnx
~~~
### 将 `onnx` 模型导回其他框架

将 `onnx` 模型导回 PyTorch：
~~~py
import torch
import onnx
import onnx2pytorch

# 加载 onnx 模型
onnx_model = onnx.load("simple_model.onnx")

# 将 onnx 模型转换为 PyTorch 模型
pytorch_model = onnx2pytorch.ConvertModel(onnx_model)
~~~

### 常见操作总结

1.  **安装和导入**：
    
~~~bash
pip install onnx onnxruntime
~~~
    
2.  **导出模型到 `onnx`**：
    
~~~py
import torch.onnx 
torch.onnx.export(model, dummy_input, "model.onnx")
~~~
3.  **加载和运行 `onnx` 模型**：
    
~~~py
import onnxruntime as ort 
ort_session = ort.InferenceSession("model.onnx") outputs = 
ort_session.run(None, {"input": input_data})
~~~
4.  **验证 `onnx` 模型**：
~~~py
import onnx 
onnx_model = onnx.load("model.onnx") 
onnx.checker.check_model(onnx_model)`
~~~
5.  **转换其他框架模型到 `onnx`**：
~~~bash
python -m tf2onnx.convert --saved-model saved_model_dir --output model.onnx
~~~

通过上述示例和操作步骤，你可以熟练掌握 `onnx` 的常见用法，从而在不同深度学习框架之间进行模型的无缝转换和部署。

`onnx.load` 和 `ort.InferenceSession` 在处理 ONNX 模型时有不同的用途和作用：

### `onnx.load`

`onnx.load` 是 `onnx` 库中的一个函数，用于加载和解析一个 ONNX 模型文件。这个函数会读取模型文件并返回一个 `onnx.ModelProto` 对象，该对象包含了模型的所有定义，包括模型的计算图、参数、节点等信息。
~~~py
import onnx

# 加载 ONNX 模型文件
onnx_model = onnx.load("model.onnx")

# 检查模型是否有效
onnx.checker.check_model(onnx_model)
~~~
#### 用途

-   **读取模型文件**：从磁盘加载一个已保存的 ONNX 模型文件。
-   **模型验证**：可以使用 `onnx.checker` 对加载的模型进行验证，确保模型符合 ONNX 的规范和标准。
-   **模型分析**：可以进一步分析模型的计算图、节点、输入输出等信息，使用 `onnx.helper` 提供的工具对模型进行操作和修改。
### `ort.InferenceSession`

`ort.InferenceSession` 是 `onnxruntime` 库中的一个类，用于创建一个推理会话 (Inference Session)。这个会话封装了对 ONNX 模型的推理过程，能够加载模型并在给定的输入数据上运行推理，得到输出结果。
~~~py
import onnxruntime as ort

# 创建 ONNX Runtime 推理会话
ort_session = ort.InferenceSession("model.onnx")

# 创建输入数据
input_data = {"input_name": input_array}

# 运行推理
outputs = ort_session.run(None, input_data)
print(outputs)
~~~
#### 用途

-   **模型加载和推理**：从磁盘加载 ONNX 模型并准备运行推理。
-   **推理执行**：在给定的输入数据上运行推理，得到模型输出结果。
-   **高效推理**：`onnxruntime` 通过优化的推理引擎提供高效的推理性能，支持 CPU、GPU 等多种硬件加速。

-   使用 `onnx.load` 可以读取和解析 ONNX 模型文件，适用于模型的验证和分析。
-   使用 `ort.InferenceSession` 可以加载 ONNX 模型并进行推理，适用于实际应用中的模型推理过程。

这两者通常结合使用，先用 `onnx.load` 验证和检查模型，再用 `ort.InferenceSession` 进行推理。

## PyTorch的.pt模型文件
在 PyTorch 中，`.pt` 文件通常是保存和加载模型的方式。以下是一些常见的操作，包括模型的保存、加载、推理和转换。

### 保存模型

在 PyTorch 中，有两种常用的保存模型的方式：

1.  只保存模型参数（推荐方式）。
2.  保存整个模型（包括模型架构和参数）。
#### 1. 保存模型参数
~~~py
import torch

# 假设我们有一个模型 `model`
# 保存模型参数
torch.save(model.state_dict(), "model_weights.pt")

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExOTY4NTU3NTQsLTEyMjE3MjU2MTYsLT
E4MjIyOTQzNzddfQ==
-->