## onnx

`onnx`（Open Neural Network Exchange）是一个开放格式，用于机器学习模型的跨框架互操作性。使用 `onnx`，你可以在不同的深度学习框架之间导出和导入模型。

下面是一些常见的 `onnx` 用法示例，包括如何安装 `onnx` 和相关库，如何导出模型到 `onnx` 格式，以及如何加载和使用 `onnx` 模型。

### 将Pytorch模型导出为 `onnx` 格式

假设我们使用 PyTorch 训练了一个简单的模型，现在我们将其导出为 `onnx` 格式。
~~~py
import torch
import torch.nn as nn
import torch.onnx

# 定义一个简单的模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 5)

    def forward(self, x):
        return self.fc(x)

# 初始化模型并生成一个输入张量
model = SimpleModel()
dummy_input = torch.randn(1, 10)

# 导出模型到 onnx 格式
torch.onnx.export(model, dummy_input, "simple_model.onnx", verbose=True)
~~~

~~~py
torch.onnx.export(model, args, f, export_params=True, verbose=False, training=torch.onnx.TrainingMode.EVAL, input_names=None, output_names=None, operator_export_type=None, opset_version=None, do_constant_folding=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False, onnx_shape_inference=False)
~~~

#### 参数说明

-   `model`：要导出的 PyTorch 模型。
-   `args`：模型的输入张量，或输入张量的元组。
-   `f`：要保存的文件名或文件对象。
-   `export_params`：是否导出模型参数。默认为 `True`。
-   `verbose`：是否打印导出过程的详细信息。默认为 `False`。
-   `training`：导出模型的模式，可以是 `TrainingMode.EVAL`（评估模式）或 `TrainingMode.TRAINING`（训练模式）。
-   `input_names`：输入节点的名称列表。
-   `output_names`：输出节点的名称列表。
-   `operator_export_type`：导出类型。
-   `opset_version`：导出的 ONNX opset 版本。
-   `do_constant_folding`：是否执行常量折叠优化。默认为 `True`。
-   `dynamic_axes`：动态轴的定义，用于支持可变长度输入。
-   `keep_initializers_as_inputs`：是否将初始化器保留为输入。默认为 `None`。
-   `custom_opsets`：自定义 opset 版本。
-   `enable_onnx_checker`：是否启用 ONNX 检查器。默认为 `True`。
-   `use_external_data_format`：是否使用外部数据格式。默认为 `False`。
-   `onnx_shape_inference`：是否在导出时执行 ONNX 形状推理。默认为 `False`。

### 加载和运行 `onnx` 模型

使用 `onnxruntime` 加载和运行 `onnx` 模型。
~~~py
import onnxruntime as ort
import numpy as np

# 加载 onnx 模型
ort_session = ort.InferenceSession("simple_model.onnx")

# 创建输入数据
input_data = np.random.randn(1, 10).astype(np.float32)

# 运行模型
outputs = ort_session.run(None, {"input.1": input_data})
print(outputs)
~~~
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE4MjIyOTQzNzddfQ==
-->