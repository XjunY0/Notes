## onnx

`onnx`（Open Neural Network Exchange）是一个开放格式，用于机器学习模型的跨框架互操作性。使用 `onnx`，你可以在不同的深度学习框架之间导出和导入模型。

下面是一些常见的 `onnx` 用法示例，包括如何安装 `onnx` 和相关库，如何导出模型到 `onnx` 格式，以及如何加载和使用 `onnx` 模型。

### 将Pytorch模型导出为 `onnx` 格式

假设我们使用 PyTorch 训练了一个简单的模型，现在我们将其导出为 `onnx` 格式。
~~~py
import torch
import torch.nn as nn
import torch.onnx

# 定义一个简单的模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 5)

    def forward(self, x):
        return self.fc(x)

# 初始化模型并生成一个输入张量
model = SimpleModel()
dummy_input = torch.randn(1, 10)

# 导出模型到 onnx 格式
torch.onnx.export(model, dummy_input, "simple_model.onnx", verbose=True)
~~~

~~~py
torch.onnx.export(model, args, f, export_params=True, verbose=False, training=torch.onnx.TrainingMode.EVAL, input_names=None, output_names=None, operator_export_type=None, opset_version=None, do_constant_folding=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False, onnx_shape_inference=False)
~~~

#### 参数说明

-   `model`：要导出的 PyTorch 模型。
-   `args`：模型的输入张量，或输入张量的元组。
-   `f`：要保存的文件名或文件对象。
-   `export_params`：是否导出模型参数。默认为 `True`。
-   `verbose`：是否打印导出过程的详细信息。默认为 `False`。
-   `training`：导出模型的模式，可以是 `TrainingMode.EVAL`（评估模式）或 `TrainingMode.TRAINING`（训练模式）。
-   `input_names`：输入节点的名称列表。
-   `output_names`：输出节点的名称列表。
-   `operator_export_type`：导出类型。
-   `opset_version`：导出的 ONNX opset 版本。
-   `do_constant_folding`：是否执行常量折叠优化。默认为 `True`。
-   `dynamic_axes`：动态轴的定义，用于支持可变长度输入。
-   `keep_initializers_as_inputs`：是否将初始化器保留为输入。默认为 `None`。
-   `custom_opsets`：自定义 opset 版本。
-   `enable_onnx_checker`：是否启用 ONNX 检查器。默认为 `True`。
-   `use_external_data_format`：是否使用外部数据格式。默认为 `False`。
-   `onnx_shape_inference`：是否在导出时执行 ONNX 形状推理。默认为 `False`。

### 加载和运行 `onnx` 模型

使用 `onnxruntime` 加载和运行 `onnx` 模型。
~~~py
import onnxruntime as ort
import numpy as np

# 加载 onnx 模型
ort_session = ort.InferenceSession("simple_model.onnx")

# 创建输入数据
input_data = np.random.randn(1, 10).astype(np.float32)

# 运行模型
outputs = ort_session.run(None, {"input.1": input_data})
print(outputs)
~~~

### 验证 `onnx` 模型

使用 `onnx` 库来验证模型的结构和正确性。
~~~py
import onnx

# 加载 onnx 模型
model = onnx.load("simple_model.onnx")

# 检查模型是否有效
onnx.checker.check_model(model)

# 打印模型的图形信息
print(onnx.helper.printable_graph(model.graph))
~~~

### 将其他框架的模型转换为 `onnx`

许多深度学习框架都支持将模型导出为 `onnx` 格式。例如，将 TensorFlow 模型导出为 `onnx`：
~~~py
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tf2onnx

# 定义一个简单的模型
model = Sequential([Dense(5, input_shape=(10,), activation='relu')])

# 将模型保存为 SavedModel 格式
model.save("simple_model")

# 将 SavedModel 转换为 onnx 格式
!python -m tf2onnx.convert --saved-model simple_model --output simple_model.onnx
~~~
### 将 `onnx` 模型导回其他框架

将 `onnx` 模型导回 PyTorch：
~~~py
import torch
import onnx
import onnx2pytorch

# 加载 onnx 模型
onnx_model = onnx.load("simple_model.onnx")

# 将 onnx 模型转换为 PyTorch 模型
pytorch_model = onnx2pytorch.ConvertModel(onnx_model)
~~~

### 常见操作总结

1.  **安装和导入**：
    
~~~bash
pip install onnx onnxruntime
~~~
    
2.  **导出模型到 `onnx`**：
    
~~~py
import torch.onnx 
torch.onnx.export(model, dummy_input, "model.onnx")
~~~
3.  **加载和运行 `onnx` 模型**：
    
~~~py
import onnxruntime as ort 
ort_session = ort.InferenceSession("model.onnx") outputs = 
ort_session.run(None, {"input": input_data})
~~~
4.  **验证 `onnx` 模型**：
~~~py
import onnx 
onnx_model = onnx.load("model.onnx") 
onnx.checker.check_model(onnx_model)`
~~~
5.  **转换其他框架模型到 `onnx`**：
~~~bash
python -m tf2onnx.convert --saved-model saved_model_dir --output model.onnx
~~~

通过上述示例和操作步骤，你可以熟练掌握 `onnx` 的常见用法，从而在不同深度学习框架之间进行模型的无缝转换和部署。

`onnx.load` 和 `ort.InferenceSession` 在处理 ONNX 模型时有不同的用途和作用：

### `onnx.load`

`onnx.load` 是 `onnx` 库中的一个函数，用于加载和解析一个 ONNX 模型文件。这个函数会读取模型文件并返回一个 `onnx.ModelProto` 对象，该对象包含了模型的所有定义，包括模型的计算图、参数、节点等信息。
~~~py
import onnx

# 加载 ONNX 模型文件
onnx_model = onnx.load("model.onnx")

# 检查模型是否有效
onnx.checker.check_model(onnx_model)
~~~
#### 用途

-   **读取模型文件**：从磁盘加载一个已保存的 ONNX 模型文件。
-   **模型验证**：可以使用 `onnx.checker` 对加载的模型进行验证，确保模型符合 ONNX 的规范和标准。
-   **模型分析**：可以进一步分析模型的计算图、节点、输入输出等信息，使用 `onnx.helper` 提供的工具对模型进行操作和修改。
### `ort.InferenceSession`

`ort.InferenceSession` 是 `onnxruntime` 库中的一个类，用于创建一个推理会话 (Inference Session)。这个会话封装了对 ONNX 模型的推理过程，能够加载模型并在给定的输入数据上运行推理，得到输出结果。
~~~py
import onnxruntime as ort

# 创建 ONNX Runtime 推理会话
ort_session = ort.InferenceSession("model.onnx")

# 创建输入数据
input_data = {"input_name": input_array}

# 运行推理
outputs = ort_session.run(None, input_data)
print(outputs)
~~~
#### 用途

-   **模型加载和推理**：从磁盘加载 ONNX 模型并准备运行推理。
-   **推理执行**：在给定的输入数据上运行推理，得到模型输出结果。
-   **高效推理**：`onnxruntime` 通过优化的推理引擎提供高效的推理性能，支持 CPU、GPU 等多种硬件加速。

-   使用 `onnx.load` 可以读取和解析 ONNX 模型文件，适用于模型的验证和分析。
-   使用 `ort.InferenceSession` 可以加载 ONNX 模型并进行推理，适用于实际应用中的模型推理过程。

这两者通常结合使用，先用 `onnx.load` 验证和检查模型，再用 `ort.InferenceSession` 进行推理。

## PyTorch的.pt模型文件
在 PyTorch 中，`.pt` 文件通常是保存和加载模型的方式。以下是一些常见的操作，包括模型的保存、加载、推理和转换。

### 保存模型

在 PyTorch 中，有两种常用的保存模型的方式：

1.  只保存模型参数（推荐方式）。
2.  保存整个模型（包括模型架构和参数）。
#### 1. 保存模型参数
~~~py
import torch

# 假设我们有一个模型 `model`
# 保存模型参数
torch.save(model.state_dict(), "model_weights.pt")
~~~
#### 2. 保存整个模型
~~~py
import torch

# 假设我们有一个模型 `model`
# 保存整个模型
torch.save(model, "model_full.pt")
~~~
### 加载模型

加载模型时需要先定义模型的结构，再加载参数或整个模型。
#### 1. 加载模型参数
~~~py
import torch
import torch.nn as nn

# 定义模型架构
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc = nn.Linear(10, 2)
    
    def forward(self, x):
        return self.fc(x)

# 实例化模型
model = MyModel()

# 加载模型参数
model.load_state_dict(torch.load("model_weights.pt"))

# 切换到评估模式
model.eval()
~~~
#### 2. 加载整个模型
~~~py
import torch

# 加载整个模型
model = torch.load("model_full.pt")

# 切换到评估模式
model.eval()
~~~
### 模型推理

加载模型后，可以使用该模型进行推理。推理时要确保模型处于评估模式（`model.eval()`），并且输入数据要在不跟踪梯度的上下文中处理。
~~~py
import torch

# 创建一些输入数据
input_tensor = torch.randn(1, 10)  # 假设输入大小为 [1, 10]

# 模型推理
with torch.no_grad():
    output = model(input_tensor)
    print(output)
~~~
### 转换为 ONNX 格式

有时候需要将 PyTorch 模型转换为 ONNX 格式以便在其他框架中使用。可以使用 `torch.onnx.export` 完成此操作。
~~~py
import torch

# 假设我们有一个模型 `model`
# 创建一个示例输入张量
dummy_input = torch.randn(1, 10)

# 导出为 ONNX 格式
torch.onnx.export(model, dummy_input, "model.onnx", 
                  input_names=["input"], output_names=["output"], 
                  dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}})
~~~


## 基于 PyTorch 模型的 ONNX 转换与推理实现

#### 介绍

在新媒体平台的实际应用中，我们可能需要将训练后的模型转换为其他格式，以支持不同的推理引擎。ONNX（Open Neural Network Exchange）提供了一个开放的源模型格式，支持多种深度学习框架之间的模型转换。在本任务中你们需要将给定的基于 PyTorch 的文本分类模型转换为 ONNX 格式，并实现一个使用 ONNX 模型进行推理的简单应用。

#### 准备

开始答题前，请确认 `/home/project` 目录下包含以下文件：

-   task.py
-   model.pt

其中：

-   `task.py`，是你后续答题过程中编写代码的地方。
-   `model.pt`，是本任务提供的 PyTorch 模型权重文件，你可以通过如下方式来使用：

```python

import torch
import torch.nn as nn

class TextClassifier(nn.Module):
    def __init__(self, vocab_size=1000, embed_dim=128, hidden_dim=512, num_classes=2):
        super(TextClassifier, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.rnn = nn.LSTM(embed_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, text):

        embedded = self.embedding(text)
        packed_output, (hidden, cell) = self.rnn(embedded)
        output = self.fc(hidden.squeeze(0))
        return output

model = TextClassifier()
model.load_state_dict(torch.load('model.pt'))
model.eval()
output = model(torch.ones([256, 1], dtype=torch.long))
print(output.detach().numpy().tolist())
# [[0.05428319424390793, 0.09556099772453308]]
```

#### 目标

请在 `task.py` 文件中根据以下要求编写代码。

**`convert` 函数**

-   功能
    -   将本任务提供的 pt 模型文件转换为 ONNX 格式。
    -   ONNX 模型文件保存在 `task.py` 文件同级目录下。
    -   ONNX 模型文件名为 `text_classifier.onnx`。

**`inference` 函数**

-   功能
    -   读取 ONNX 模型文件。
    -   使用 ONNX 模型进行推理。
    -   返回推理结果。
-   参数
    -   model_path，字符串类型，ONNX 模型文件的绝对路径。
    -   input，整数列表类型，为待预测样本，示例如：[101, 304, 993, 1008,102]。
-   返回值
    -   result，浮点数列表类型，为 ONNX 文件推理结果，示例如：[[0.53419, 0.44313]]。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTg2ODU0NjA4OSw1ODU5MjA1OTYsLTEyMj
E3MjU2MTYsLTE4MjIyOTQzNzddfQ==
-->