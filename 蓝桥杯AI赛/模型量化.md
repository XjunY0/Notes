
模型量化是深度学习中一种优化技术，旨在减少模型大小和提高推理速度，同时在精度上尽量不造成太大的损失。量化通过将模型参数从高精度（如 32 位浮点数）转换为低精度（如 8 位整数），从而实现上述目标。量化特别适合在资源受限的设备（如移动设备和嵌入式系统）上运行的模型。

### 模型量化的类型

1.  **权重量化（Weight Quantization）**：仅对模型权重进行量化。
2.  **激活量化（Activation Quantization）**：对模型权重和激活值进行量化。
3.  **动态范围量化（Dynamic Range Quantization）**：在推理时将激活值量化为 8 位整数。
4.  **完全量化（Full Integer Quantization）**：将权重和激活值都量化为整数，包括推理过程中的所有计算。
5.  **混合量化（Float16 Quantization）**：将权重转换为 16 位浮点数。
### 使用 TensorFlow 实现模型量化

#### 1. 动态范围量化

动态范围量化是最简单的一种量化方法，只在推理时将激活值量化为 8 位整数。

~~~py
import tensorflow as tf

# 加载一个预训练的模型
model = tf.keras.applications.MobileNetV2(weights='imagenet')

# 将模型保存为 SavedModel 格式
model.save('saved_model')

# 使用 TFLiteConverter 进行动态范围量化
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# 将模型转换为 TensorFlow Lite 格式
tflite_model = converter.convert()

# 将量化后的模型保存为 .tflite 文件
with open('model_dynamic_range_quant.tflite', 'wb') as f:
    f.write(tflite_model)
~~~

#### 2. 完全量化

完全量化会量化权重和激活值，并且在推理过程中所有计算都使用整数。
~~~py
import tensorflow as tf

# 加载一个预训练的模型
model = tf.keras.applications.MobileNetV2(weights='imagenet')

# 将模型保存为 SavedModel 格式
model.save('saved_model')

# 使用 TFLiteConverter 进行完全量化
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# 设置校准数据集（通常使用一小部分训练数据）
def representative_dataset():
    for _ in range(100):
        data = np.random.random((1, 224, 224, 3)).astype(np.float32)
        yield [data]

converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # 或者 tf.uint8
converter.inference_output_type = tf.int8  # 或者 tf.uint8

# 将模型转换为 TensorFlow Lite 格式
tflite_model = converter.convert()

# 将量化后的模型保存为 .tflite 文件
with open('model_full_integer_quant.tflite', 'wb') as f:
    f.write(tflite_model)
~~~

#### 3. 混合量化（Float16 量化）

混合量化将权重转换为 16 位浮点数，以减小模型大小，同时保持较高的精度。
~~~py
import tensorflow as tf

# 加载一个预训练的模型
model = tf.keras.applications.MobileNetV2(weights='imagenet')

# 将模型保存为 SavedModel 格式
model.save('saved_model')

# 使用 TFLiteConverter 进行混合量化
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]

# 将模型转换为 TensorFlow Lite 格式
tflite_model = converter.convert()

# 将量化后的模型保存为 .tflite 文件
with open('model_float16_quant.tflite', 'wb') as f:
    f.write(tflite_model)
~~~

### 总结
### 1. 权重量化（Weight Quantization）：

使用 TensorFlow Lite 转换器的 `optimizations` 参数，将其设置为 `[tf.lite.Optimize.DEFAULT]`，默认情况下会应用权重量化。
~~~py
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
~~~

### 2. 激活量化（Activation Quantization）：
激活量化通常与权重量化一起使用，只需设置 TensorFlow Lite 转换器的 `optimizations` 参数为 `[tf.lite.Optimize.DEFAULT]`，默认情况下会同时应用激活量化和权重量化。
~~~py
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
~~~

### 3. 动态范围量化（Dynamic Range Quantization）：

动态范围量化可以通过设置 TensorFlow Lite 转换器的 `optimizations` 参数为 `[tf.lite.Optimize.DEFAULT]` 来实现，这也是默认的优化选项。

~~~py
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
~~~

### 4. 完全量化（Full Integer Quantization）：

完全量化需要将 TensorFlow Lite 转换器的 `target_spec.supported_ops` 参数设置为 `[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`，同时设置输入和输出数据类型为整数。
~~~py
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
~~~

### 5. 混合量化（Mixed Precision Quantization）：

混合量化通常会将模型权重转换为低精度的浮点数（如 16 位浮点数）。这可以通过设置转换器的 `target_spec.supported_types` 参数为 `[tf.float16]` 来实现。
~~~py
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

converter.target_spec.supported_types = [tf.float16]
~~~
## 实现模型量化

#### 介绍

在深度学习模型的部署过程中，模型的大小和运行效率是非常重要的因素。TensorFlow Lite 是一个用于移动和嵌入式设备的开源深度学习框架，它可以将模型大小减小，并提高运行效率。现在，你们已经完成了虚假新闻模型训练，接下来你们小组将继续完成模型的量化。

#### 准备

开始答题前，请确认 `/home/project` 目录下包含以下文件：

-   model.h5
    
-   word_index.json
    
-   task.py
    
    其中：
    
-   `model.h5` 是本任务提供的基于 Tensorflow.keras 训练的可用于虚假新闻识别的二分类模型。
    
-   `word_index.json` 是在模型训练之前基于训练集生成的词汇表。在本任务中，你可以使用它将测试数据转换为 id 序列。关于词汇表的生成，可参考如下代码片段。
    
    ```python
    from tensorflow.keras.preprocessing.text import Tokenizer
    tokenizer = Tokenizer()
    # 生成词汇表，texts 是训练集（若干行分词后的句子）
    tokenizer.fit_on_texts(texts) 
    # 获取词汇表
    word_index = tokenizer.word_index
    # 保存词汇表
    with open('word_index.json', 'w', encoding='utf-8') as f:
        json.dump(word_index, f, ensure_ascii=False)
    ```
    
-   `task.py`，是你后续答题过程中编写代码的地方。
    

#### 目标

请在 `task.py` 文件中按要求实现以下目标函数。

**`quantize_model` 函数**

-   功能
    -   使用 TensorFlow Lite 的 TFLiteConverter 进行量化。
    -   将训练好的 Keras 模型的所有参数量化为 16 位浮点数。
    -   将量化后的模型保存为 TFLite 格式，模型大小不超过 4MB。
-   参数
    -   model_path (str): 已经训练好的模型的文件路径。
    -   quantized_model_path (str): 量化后的模型的保存路径。
-   返回值：无

**`prediction_label` 函数**

-   功能
    -   使用量化模型对给定的句子进行预测，并返回预测的类别标签。
        
        > 提示：模型预测前需将句子处理成长度 100 的序列。
        
-   参数
    -   test_sentence (str): 需要进行预测的句子。
    -   model_path (str): 量化后的模型的文件路径。
-   返回值
    -   prediction_label (int): 预测的类别标签，值为 0 或 1。
~~~py
def quantize_model(model_path, quantized_model_path):

#TODO

model = tf.keras.models.load_model('model.h5')

converter = tf.lite.TFLiteConverter.from_keras_model(model)

converter.target_spec.supported_types = [tf.float16]

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

converter._experimental_lower_tensor_list_ops = False

litemodel = converter.convert()

with open(quantized_model_path,'wb') as f:

f.write(litemodel)

  
  

def prediction_label(test_sentence, model_path):

# TODO

with open('word_index.json','r') as f:

	word_index = json.load(f)

tokenizer = Tokenizer()

tokenizer.word_index = word_index

data_train = tokenizer.texts_to_sequences([test_sentence])

data_train = pad_sequences(data_train,100)

data_train = tf.cast(data_train,tf.float32)

  

interpreter = tf.lite.Interpreter(model_path)

interpreter.allocate_tensors()

input_details = interpreter.get_input_details()[0]

output_details = interpreter.get_output_details()[0]

interpreter.set_tensor(input_details['index'],data_train)

interpreter.invoke()

predict = interpreter.get_tensor(output_details['index'])

predict = (predict>0.5).astype(np.int_)

return predict[0][0]
~~~

具体见[官方文档](https://tensorflow.google.cn/model_optimization/guide/quantization/post_training?hl=zh-cn)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTc4NzQxODEyMCwxMDk1MTg1ODI0LC05NT
AyNjU4NTgsMjMyNDM4MDIyLDIxMjQ4MzMyNzIsLTExODMzNTQy
MjldfQ==
-->