 
## 1.设计并训练假新闻过滤模型（LSTM）

#### 介绍

新媒体作为信息传播的重要渠道，需确保传播的信息可信可靠。因此，平台需要采取假新闻过滤机制限制虚假信息的传播，提高内容质量。在本任务中，你将基于处理好的数据集，设计并构建一个针对新闻文本的真假分类模型。

#### 准备

开始答题前，请确认 `/home/project` 目录下包含以下文件：

-   news_train.txt
    
-   label_newstrain.txt
    
-   news_test.txt
    
-   task.py
    
    其中：
    
-   `news_train.txt`，是本任务提供的训练集文本。文件中每行是一个样本，即经过分词处理后的句子。
    
-   `label_newstrain.txt`，是本任务提供的训练集标签。文件中每行是一个 0/1 数值标签，与 `news_train.txt` 文件逐行一一对应。0 表示文本真实，1 则表示虚假。
    
-   `news_test.txt`，是本任务提供的测试集文本。文件中每行是一个样本，即经过分词处理后的句子。
    
-   `task.py`，是你后续答题过程中编写代码的地方。
    

#### 目标

请在 `task.py` 文件中编写代码，并按以下要求实现对新闻文本的真假进行自动预测。

① 选择合适的分类算法，构建一个分类模型。  
② 使用训练好的分类模型预测测试集 `news_test.txt` 的类别标签。  
③ 将输出结果保存在 `task.py` 文件同级目录下，命名为 `pred_test.txt`。注意：结果文件中只保存 0/1 标签，每行一个数值，第一行的数值对应 `news_test.txt` 中第一行文本的真假类别，以此类推。  
④ `pred_test.txt` 结果准确率不低于 0.9，视为实现目标。

> 提示：判题过程中的准确率计算示例：真实标签为 [0, 1, 0, 1, 1]，预测标签为 [0, 1, 1, 1, 0]，准确率为 0.6，表示 60% 的样本被正确预测。

 1. 导入必要的库
~~~python
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
~~~

 2. 从文件读取数据
~~~python
open('news_train.txt', 'r', encoding='utf-8') as f:
	news_train = f.readlines() 
with open('label_newstrain.txt', 'r') as f: 
	labels_train = [int(line.strip()) for line in f.readlines()]
~~~
- `readlines()`将每一行的数据读入一个列表中
- `line.strip()`丢掉换行符

3. Tokenize和向量化文本数据
~~~python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(news_train + news_test)
X_train = tokenizer.texts_to_sequences(news_train)
X_test = tokenizer.texts_to_sequences(news_test)
~~~
-   `Tokenizer` 是 Keras 库中用于将文本数据转换为数值数据的工具。通过这种转换，文本数据可以更容易地输入到神经网络模型中进行训练。
-   `fit_on_texts` 方法会遍历提供的所有文本数据，并为其中的每个唯一词语分配一个唯一的整数索引。这样，`Tokenizer` 就建立了一个词汇表，将词语映射到整数。
-   `texts_to_sequences(news_train)`：`texts_to_sequences` 方法将训练集 `news_train` 中的每个文本转换为一个整数序列。每个整数代表 `Tokenizer` 中的一个词汇表索引。例如，假设词汇表中单词 "hello" 的索引是 1，"world" 的索引是 2，那么句子 "hello world" 会被转换为 [1, 2]。

4. 对序列进行padding 让个句子序列长度相等
~~~python
max_len = max(len(seq) for seq in X_train + X_test)
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)
~~~
-   `max_len`：计算所有序列的最大长度，确保所有序列统一长度。
- `pad_sequences` 是 Keras 提供的一个函数，用于将不同长度的序列填充到相同的长度。这里具体的操作包括：
	1. `X_train` 中的每个序列都会被填充（或截断）到长度 `max_len`。
	2. 如果一个序列的长度小于 `max_len`，那么在序列的开始位置会填充零（默认情况下）以达到 `max_len` 的长度。
	3. 如果一个序列的长度大于 `max_len`，则序列会被截断，只保留最后的 `max_len` 个元素。

	填充后的 `X_train` 是一个二维数组，每个序列的长度都是 `max_len`。
	### 背后的概念

-   **序列填充（Padding）：** 由于神经网络通常要求输入数据形状一致（特别是 RNN、LSTM 等模型），所以需要将所有序列填充到相同的长度。这有助于批量处理数据，提高计算效率。
-   **填充方式：** 默认情况下，`pad_sequences` 函数在序列的开始位置填充零（这种方式称为前填充）。也可以通过设置 `padding='post'` 参数在序列末尾进行填充。
-   **截断方式：** 如果序列长度超过 `max_len`，默认会保留最后 `max_len` 个元素（这种方式称为后截断）。也可以通过设置 `truncating='pre'` 参数截断序列的开始部分。

5. 将标签转换为NumPy数组
~~~python
labels_train = np.array(labels_train)
~~~
-   `np.array(labels_train)`：将标签列表转换为NumPy数组，以便Keras模型处理。


6. 利用keras构建LSTM模型
~~~python
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128))
model.add(LSTM(128))
model.add(Dense(1, activation='sigmoid'))
~~~
- `Sequential` 模型是 Keras 中最简单的模型，它允许我们逐层添加神经网络层，每层的输出作为下一层的输入。
- 嵌入层的作用是将输入的词汇索引转换为稠密的向量表示。这行代码的具体含义如下：

	1. `input_dim=len(tokenizer.word_index) + 1`：指定嵌入层的输入维度，即词汇表的大小。由于 `Tokenizer` 中的词汇索引从 1 开始，而 0 是留给填充值的，所以这里需要加 1。
	2. `output_dim=128`：指定嵌入向量的维度，每个词汇索引将被映射为一个 128 维的向量。
- `LSTM`（长短期记忆网络）层是处理序列数据的关键层，能够捕捉序列中的长期依赖关系。这里我们添加了一个有 128 个隐藏单元的 LSTM 层。它会处理来自嵌入层的输出，并将其传递到下一层。
	-  LSTM 层的维度推断

	1.  输入维度：
	    
	    -   嵌入层的输出形状为 `(batch_size, sequence_length, embedding_dim)`。
	    -   在这个例子中，假设批量大小为 `batch_size`，序列长度为 `sequence_length`，嵌入维度为 `embedding_dim`。
	2.  LSTM 层的输入：
	    
	    -   当你将嵌入层的输出传递给 LSTM 层时，LSTM 层会接收到形状为 `(batch_size, sequence_length, embedding_dim)` 的输入。
	    -   Keras 会**自动**推断出 `embedding_dim` 作为 LSTM 层的输入特征维度。
	3.  LSTM 层的输出：
	    
	    -   `LSTM(units)` 中的 `units` 参数指定了 LSTM 层的**输出**维度，即隐藏单元的数量。
	    -   如果 `units=128`，则 LSTM 层会输出一个形状为 `(batch_size, 128)` 的张量（假设 `return_sequences=False`）。
	    -   如果 `return_sequences=True`，则 LSTM 层会输出一个形状为 `(batch_size, sequence_length, 128)` 的张量，其中每个时间步的输出都有 `128` 个特征

- `Dense` (全连接层）通常作为网络的输出层。这里的 Dense 层有一个神经元，意味着网络最终输出一个值。`activation='sigmoid'` 表示我们使用 sigmoid 激活函数，将输出值映射到 [0, 1] 区间。这在二分类问题中（例如，情感分析中的正面/负面分类）特别有用，因为 sigmoid 激活函数可以直接输出一个概率值。


7. 编译模型
~~~python
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
~~~
- **优化器（Optimizer）**：`adam` 是一种流行的优化算法，它结合了 AdaGrad 和 RMSProp 的优点，适用于大多数深度学习模型。
	1. Adam（自适应矩估计）**：使用动态学习率来调整每个参数的学习步伐。Adam 优化器通过计算梯度的一阶和二阶矩估计来更新模型参数，这样可以在训练过程中更好地适应不同特征的尺度。
	2. 参数：
    -   `learning_rate`（学习率）：控制模型参数更新的步长，默认值通常为 0.001。
    -   `beta_1` 和 `beta_2`：分别控制一阶和二阶矩估计的指数衰减率，默认值分别为 0.9 和 0.999。
- **损失函数（Loss function）**：`binary_crossentropy` 是二分类问题常用的损失函数。
	1. 二元交叉熵（Binary Cross-Entropy）**：用于衡量模型预测概率和实际标签之间的差异。
	2. 应用场景：适用于输出层使用 `sigmoid` 激活函数的二分类问题。
- **评估指标（Metrics）**：`accuracy` 是一种常用的评估指标，用于衡量模型的预测准确率。


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTI0NDg2MzE5OCwxMDg4NzgxMTE5LC0xOD
k4Mzc0MTg2LC0xMTM0ODYwMTI1XX0=
-->