 
## 1.设计并训练假新闻过滤模型

#### 介绍

新媒体作为信息传播的重要渠道，需确保传播的信息可信可靠。因此，平台需要采取假新闻过滤机制限制虚假信息的传播，提高内容质量。在本任务中，你将基于处理好的数据集，设计并构建一个针对新闻文本的真假分类模型。

#### 准备

开始答题前，请确认 `/home/project` 目录下包含以下文件：

-   news_train.txt
    
-   label_newstrain.txt
    
-   news_test.txt
    
-   task.py
    
    其中：
    
-   `news_train.txt`，是本任务提供的训练集文本。文件中每行是一个样本，即经过分词处理后的句子。
    
-   `label_newstrain.txt`，是本任务提供的训练集标签。文件中每行是一个 0/1 数值标签，与 `news_train.txt` 文件逐行一一对应。0 表示文本真实，1 则表示虚假。
    
-   `news_test.txt`，是本任务提供的测试集文本。文件中每行是一个样本，即经过分词处理后的句子。
    
-   `task.py`，是你后续答题过程中编写代码的地方。
    

#### 目标

请在 `task.py` 文件中编写代码，并按以下要求实现对新闻文本的真假进行自动预测。

① 选择合适的分类算法，构建一个分类模型。  
② 使用训练好的分类模型预测测试集 `news_test.txt` 的类别标签。  
③ 将输出结果保存在 `task.py` 文件同级目录下，命名为 `pred_test.txt`。注意：结果文件中只保存 0/1 标签，每行一个数值，第一行的数值对应 `news_test.txt` 中第一行文本的真假类别，以此类推。  
④ `pred_test.txt` 结果准确率不低于 0.9，视为实现目标。

> 提示：判题过程中的准确率计算示例：真实标签为 [0, 1, 0, 1, 1]，预测标签为 [0, 1, 1, 1, 0]，准确率为 0.6，表示 60% 的样本被正确预测。

 1. 导入必要的库
~~~python
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
~~~

 2. 从文件读取数据
~~~python
open('news_train.txt', 'r', encoding='utf-8') as f:
	news_train = f.readlines() 
with open('label_newstrain.txt', 'r') as f: 
	labels_train = [int(line.strip()) for line in f.readlines()]
~~~
- `readlines()`将每一行的数据读入一个列表中
- `line.strip()`丢掉换行符

3. Tokenize和向量化文本数据
~~~python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(news_train + news_test)
X_train = tokenizer.texts_to_sequences(news_train)
X_test = tokenizer.texts_to_sequences(news_test)
~~~
-   `Tokenizer` 是 Keras 库中用于将文本数据转换为数值数据的工具。通过这种转换，文本数据可以更容易地输入到神经网络模型中进行训练。
-   `fit_on_texts` 方法会遍历提供的所有文本数据，并为其中的每个唯一词语分配一个唯一的整数索引。这样，`Tokenizer` 就建立了一个词汇表，将词语映射到整数。
-   `texts_to_sequences(news_train)`：将训练集的文本数据转换为整数序列，每个单词对应一个整数。
-   `texts_to_sequences(news_test)`：将测试集的文本数据转换为整数序列。
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTg0NDQzODQyOCwtMTEzNDg2MDEyNV19
-->